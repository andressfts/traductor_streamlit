import streamlit as st
import torch
from transformers import MarianTokenizer, MarianMTModel

# Forzar uso de CPU (Streamlit Cloud no tiene GPU)
device = torch.device("cpu")

# Ruta del modelo personalizado entrenado
model_path = "./modelo_personalizado"

# Cargar tokenizer base y modelo entrenado (NO usar .to(device))
tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-es-en")
model = MarianMTModel.from_pretrained(model_path)

# Interfaz de usuario
st.title("Traductor Personalizado Espa√±ol ‚Üí Ingl√©s")
st.write("Este traductor usa un modelo entrenado con frases espec√≠ficas.")

# Entrada del usuario
texto = st.text_area("Escribe en espa√±ol:", "Buenos d√≠as")

if st.button("Traducir"):
    if texto.strip():
        with st.spinner("Traduciendo..."):
            try:
                # Tokenizar entrada y mover tensores a CPU
                tokens = tokenizer(texto, return_tensors="pt", padding=True, truncation=True)
                tokens = {k: v.to("cpu") for k, v in tokens.items()}  # üëà aqu√≠ expl√≠citamente "cpu"

                # Generar traducci√≥n
                traduccion_ids = model.generate(**tokens)
                traduccion = tokenizer.decode(traduccion_ids[0], skip_special_tokens=True)

                # Mostrar resultado
                st.success("‚úÖ Traducci√≥n completada")
                st.text_area("Traducci√≥n:", traduccion, height=100)
            except Exception as e:
                st.error(f"‚ùå Error durante la traducci√≥n: {e}")
    else:
        st.warning("‚ö†Ô∏è Por favor ingresa un texto.")
